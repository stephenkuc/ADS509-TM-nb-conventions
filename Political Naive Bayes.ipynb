{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes on Political Text\n",
    "### Stephen Kuc\n",
    "### ADS509\n",
    "\n",
    "In this notebook we use Naive Bayes to explore and classify political data. See the `README.md` for full details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import nltk\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import Counter, defaultdict\n",
    "import pandas as pd\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "\n",
    "# Feel free to include your text patterns functions\n",
    "# from text_function_solutions import descriptive_stats, remove_stop,  tokenize, prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "import re\n",
    "\n",
    "\n",
    "# Some punctuation variations\n",
    "punctuation = set(punctuation) # speeds up comparison\n",
    "tw_punct = punctuation - {\"#\"}\n",
    "\n",
    "# Stopwords\n",
    "sw = stopwords.words(\"english\")\n",
    "sw = set(sw)\n",
    "\n",
    "# Two useful regex\n",
    "whitespace_pattern = re.compile(r\"\\s+\")\n",
    "hashtag_pattern = re.compile(r\"^#[0-9a-zA-Z]+\")\n",
    "\n",
    "# and now our functions\n",
    "def descriptive_stats(tokens, num_tokens = 5, verbose=True) :\n",
    "\n",
    "    \n",
    "    # finding total number of tokens by the length of tokens list\n",
    "    num_tokens = len(tokens)\n",
    "    \n",
    "    # creating a set of tokens to find unique, and then length of that\n",
    "    num_unique_tokens = len(set(tokens))\n",
    "    \n",
    "    # dividing unique over total to find diversity\n",
    "    lexical_diversity = num_unique_tokens / num_tokens\n",
    "    \n",
    "    \n",
    "    # creating a quick for-loop to cound all characters in document\n",
    "    num_characters_list = []\n",
    "    for word in tokens:\n",
    "        num_characters_list.append(len(word))\n",
    "        \n",
    "    num_characters = sum(num_characters_list)\n",
    "    \n",
    "    # utilizing Counter to count the characters and then finding the top 5\n",
    "    \n",
    "    common_count = Counter(tokens)\n",
    "    \n",
    "    most_common_five = common_count.most_common(5)\n",
    "    \n",
    "    if verbose == True:        \n",
    "        print(f\"There are {num_tokens} tokens in the data.\")\n",
    "        print(f\"There are {num_unique_tokens} unique tokens in the data.\")\n",
    "        print(f\"There are {num_characters} characters in the data.\")\n",
    "        print(f\"The lexical diversity is {lexical_diversity:.3f} in the data.\")\n",
    "    \n",
    "        # print the five most common tokens\n",
    "        print(f\"These are the five most common tokens and their count:\\n {most_common_five}\")\n",
    "        \n",
    "    return([num_tokens, num_unique_tokens,\n",
    "            lexical_diversity,\n",
    "            num_characters, most_common_five])\n",
    "\n",
    "    \n",
    "def remove_stop(tokens) :\n",
    "    \n",
    "    return [t for t in tokens if t.lower() not in sw]\n",
    " \n",
    "def remove_punctuation(text, punct_set=tw_punct) : \n",
    "    return(\"\".join([ch for ch in text if ch not in punct_set]))\n",
    "\n",
    "def tokenize(text) : \n",
    "    text = text.split(' ') # split on whitespace to include hashtags and other information\n",
    "    return(text)\n",
    "\n",
    "def prepare(text, pipeline) : \n",
    "    tokens = str(text)\n",
    "    \n",
    "    for transform in pipeline : \n",
    "        tokens = transform(tokens)\n",
    "        \n",
    "    return(tokens)\n",
    "\n",
    "def join_tokens(tokens):\n",
    "    \n",
    "    text = \" \".join(tokens)\n",
    "    \n",
    "    return(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "convention_db = sqlite3.connect(\"2020_Conventions.db\") # we are in the same directory\n",
    "convention_cur = convention_db.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Exploratory Naive Bayes\n",
    "\n",
    "We'll first build a NB model on the convention data itself, as a way to understand what words distinguish between the two parties. This is analogous to what we did in the \"Comparing Groups\" class work. First, pull in the text \n",
    "for each party and prepare it for use in Naive Bayes.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "convention_data = []\n",
    "\n",
    "# fill this list up with items that are themselves lists. The \n",
    "# first element in the sublist should be the cleaned and tokenized\n",
    "# text in a single string. The second element should be the party. \n",
    "\n",
    "query_results = convention_cur.execute(\"SELECT text, party FROM conventions;\")\n",
    "\n",
    "my_pipeline = [str.lower, remove_punctuation, tokenize, remove_stop, join_tokens]\n",
    "\n",
    "convention_table = []\n",
    "\n",
    "for row in query_results :\n",
    "    text, party = row\n",
    "    \n",
    "    convention_table.append((text, party))\n",
    "\n",
    "    \n",
    "    \n",
    "convention_df = pd.DataFrame(convention_table, columns = [\"text\", \"party\"])\n",
    "\n",
    "tokens = convention_df['text'].apply(prepare, pipeline = my_pipeline)\n",
    "\n",
    "convention_df['text'] = tokens\n",
    "\n",
    "convention_data = convention_df.values.tolist()\n",
    "\n",
    "# there must be a better way to do this \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at some random entries and see if they look right. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['tennessee', 'Democratic'],\n",
       " ['good evening it’s honor come tonight commonwealth kentucky leader washington either new york california consider responsibility look middle america election incredibly consequential middle america president trump knows inherited first generation americans couldn’t promise children better life he’s made mission administration change know work beside every day today’s democrat party doesn’t want improve life middle america prefer us flyover country keep quiet let decide live lives',\n",
       "  'Republican'],\n",
       " ['let us pray pray must grateful citizens country boldly claim one nation god pray must praising lord country freedom religion cherished republicans democrats begin conventions heads bowed prayer pray must conscious suffering covid unwearied frontliners care us pray must lives may protected respected troubled cities police guard tense world situations men women uniform keep peace innocent life baby womb elders nursing care hospice immigrants refugees lives threatened religious persecution throughout world plague hunger drugs human trafficking war pray must thanksgiving thanksgiving dear god democracy ask hand almighty father upon convention nominees parties wisdom upon electorate eager perform duty faithful citizenship pray dare claim god trust amen',\n",
       "  'Republican'],\n",
       " ['100 years ago tonight suffragists based hermitage hotel nashville cheered tennessee became 36th deciding state ratify 19th amendment granting women right vote year i’m casting first presidential vote joe biden women decide election replace donald trump president respects us tennessee cast 23 votes bernie sanders 50 votes next president united states mr joseph r biden',\n",
       "  'Democratic'],\n",
       " ['good evening folks i’m tony evers i’m incredibly proud 46th governor great state wisconsin really looking forward america’s dairyland unfortunately pandemic means can’t year unites us far far greater divides us even though can’t know shared sense purpose elect joe biden kamala harris november returning kindness respect empathy civility back white house that’s joe kamala know especially challenging times like problems face solved us together holy macro folks let’s get work',\n",
       "  'Democratic'],\n",
       " ['family complete', 'Democratic'],\n",
       " ['new hampshire', 'Democratic'],\n",
       " ['openly gay man georgia state legislature', 'Democratic'],\n",
       " ['want freedom gradually want free', 'Democratic'],\n",
       " ['people say “we’re oppressed united states”', 'Republican']]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.choices(convention_data,k=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If that looks good, we now need to make our function to turn these into features. In my solution, I wanted to keep the number of features reasonable, so I only used words that occur at least `word_cutoff` times. Here's the code to test that if you want it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With a word cutoff of 5, we have 2391 as features in the model.\n"
     ]
    }
   ],
   "source": [
    "word_cutoff = 5\n",
    "\n",
    "tokens = [w for t, p in convention_data for w in t.split()]\n",
    "\n",
    "word_dist = nltk.FreqDist(tokens)\n",
    "\n",
    "feature_words = set()\n",
    "\n",
    "for word, count in word_dist.items() :\n",
    "    if count > word_cutoff :\n",
    "        feature_words.add(word)\n",
    "        \n",
    "print(f\"With a word cutoff of {word_cutoff}, we have {len(feature_words)} as features in the model.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_features(text,fw) :\n",
    "    \"\"\"Given some text, this returns a dictionary holding the\n",
    "       feature words.\n",
    "       \n",
    "       Args: \n",
    "            * text: a piece of text in a continuous string. Assumes\n",
    "            text has been cleaned and case folded.\n",
    "            * fw: the *feature words* that we're considering. A word \n",
    "            in `text` must be in fw in order to be returned. This \n",
    "            prevents us from considering very rarely occurring words.\n",
    "        \n",
    "       Returns: \n",
    "            A dictionary with the words in `text` that appear in `fw`. \n",
    "            Words are only counted once. \n",
    "            If `text` were \"quick quick brown fox\" and `fw` = {'quick','fox','jumps'},\n",
    "            then this would return a dictionary of \n",
    "            {'quick' : True,\n",
    "             'fox' :    True}\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    ret_dict = dict()\n",
    "    \n",
    "    words = text.split(\" \") # splitting words on white space to create as tokens again\n",
    "    \n",
    "    for w in words: \n",
    "        if w in fw: # checking if word is included in feature words\n",
    "            ret_dict[w] = True\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    return(ret_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(len(feature_words)>0)\n",
    "assert(conv_features(\"donald is the president\",feature_words)==\n",
    "       {'donald':True,'president':True})\n",
    "assert(conv_features(\"people are american in america\",feature_words)==\n",
    "                     {'america':True,'american':True,\"people\":True})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll build our feature set. Out of curiosity I did a train/test split to see how accurate the classifier was, but we don't strictly need to since this analysis is exploratory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresets = [(conv_features(text,feature_words), party) for (text, party) in convention_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(20220507)\n",
    "random.shuffle(featuresets)\n",
    "\n",
    "test_size = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n"
     ]
    }
   ],
   "source": [
    "test_set, train_set = featuresets[:test_size], featuresets[test_size:]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print(nltk.classify.accuracy(classifier, test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                   china = True           Republ : Democr =     25.8 : 1.0\n",
      "                   votes = True           Democr : Republ =     23.8 : 1.0\n",
      "             enforcement = True           Republ : Democr =     21.5 : 1.0\n",
      "                 destroy = True           Republ : Democr =     19.2 : 1.0\n",
      "                freedoms = True           Republ : Democr =     18.2 : 1.0\n",
      "                 climate = True           Democr : Republ =     17.8 : 1.0\n",
      "                supports = True           Republ : Democr =     17.1 : 1.0\n",
      "                   crime = True           Republ : Democr =     16.1 : 1.0\n",
      "                   media = True           Republ : Democr =     14.9 : 1.0\n",
      "                 beliefs = True           Republ : Democr =     13.0 : 1.0\n",
      "               countries = True           Republ : Democr =     13.0 : 1.0\n",
      "                 defense = True           Republ : Democr =     13.0 : 1.0\n",
      "                    isis = True           Republ : Democr =     13.0 : 1.0\n",
      "                 liberal = True           Republ : Democr =     13.0 : 1.0\n",
      "                religion = True           Republ : Democr =     13.0 : 1.0\n",
      "                   trade = True           Republ : Democr =     12.7 : 1.0\n",
      "                    flag = True           Republ : Democr =     12.1 : 1.0\n",
      "               greatness = True           Republ : Democr =     12.1 : 1.0\n",
      "                 abraham = True           Republ : Democr =     11.9 : 1.0\n",
      "                  defund = True           Republ : Democr =     11.9 : 1.0\n",
      "                    drug = True           Republ : Democr =     10.9 : 1.0\n",
      "              department = True           Republ : Democr =     10.9 : 1.0\n",
      "               destroyed = True           Republ : Democr =     10.9 : 1.0\n",
      "                   enemy = True           Republ : Democr =     10.9 : 1.0\n",
      "               amendment = True           Republ : Democr =     10.3 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier.show_most_informative_features(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write a little prose here about what you see in the classifier. Anything odd or interesting?\n",
    "\n",
    "### My Observations\n",
    "\n",
    "These are very interesting. Knowing that these were in 2020, when COVID was at its peak, or at least near its peak, it is pretty apparent that it was an issue talked about. The Republicans seem to like to blame China, or at least utilize China as, presumably, an enemy, although, we can't fully come to that that's how they're using \"China\" in their speeches with just this information. \n",
    "\n",
    "23 of the 25 most important features were all with Republicans saying a certain word at least 10x more than Democrats. So, it seems that they utilize these American-like buzz words, like \"destroy\", \"freedoms\", \"beliefs\", \"liberal\" \"religion\", \"flag\", to try to incite some more pride and possibly nationalism within their supporters. They also mentioned \"enemy\", \"isis\", and again \"china\". These words also seem a bit divisive.\n",
    "\n",
    "Perhaps, that is a bit negative on the Republican party, but that's what this data seems to point to. \n",
    "\n",
    "Within the Democrats, the two words in the top 25 are climate and votes. The democrats seem to utilize less distinct words, and focus in on matters like the climate. They also are probably lobbying for votes and for people to go vote, because perhaps they felt at this point in time, that was the only way they'd lose to the Republicans.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Classifying Congressional Tweets\n",
    "\n",
    "In this part we apply the classifer we just built to a set of tweets by people running for congress\n",
    "in 2018. These tweets are stored in the database `congressional_data.db`. That DB is funky, so I'll\n",
    "give you the query I used to pull out the tweets. Note that this DB has some big tables and \n",
    "is unindexed, so the query takes a minute or two to run on my machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "cong_db = sqlite3.connect(\"congressional_data.db\")\n",
    "cong_cur = cong_db.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = cong_cur.execute(\n",
    "        '''\n",
    "           SELECT DISTINCT \n",
    "                  cd.candidate, \n",
    "                  cd.party,\n",
    "                  tw.tweet_text\n",
    "           FROM candidate_data cd \n",
    "           INNER JOIN tweets tw ON cd.twitter_handle = tw.handle \n",
    "               AND cd.candidate == tw.candidate \n",
    "               AND cd.district == tw.district\n",
    "           WHERE cd.party in ('Republican','Democratic') \n",
    "               AND tw.tweet_text NOT LIKE '%RT%'\n",
    "        ''')\n",
    "\n",
    "results = list(results) # Just to store it, since the query is time consuming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_data = []\n",
    "\n",
    "tweet_table = []\n",
    "\n",
    "\n",
    "# Note that this may take a bit of time, since we have a lot of tweets.\n",
    "for row in results :\n",
    "    candidate, party, text = row\n",
    "    \n",
    "    tweet_table.append((text, party))\n",
    "\n",
    "# Now fill up tweet_data with sublists like we did on the convention speeches.\n",
    "\n",
    "# making table into df to apply pipeline on column Series objectg\n",
    "\n",
    "tweet_df = pd.DataFrame(tweet_table, columns = [\"text\", \"party\"])\n",
    "\n",
    "tokens = tweet_df['text'].apply(prepare, pipeline = my_pipeline)\n",
    "\n",
    "tweet_df['text'] = tokens\n",
    "\n",
    "tweet_data = tweet_df.values.tolist()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a lot of tweets here. Let's take a random sample and see how our classifer does. I'm guessing it won't be too great given the performance on the convention speeches..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['bmy thoughts affected terrible fires burning across californiannthank first responders working tirelessly keep us safe httpstcojnphimfh5l',\n",
       "  'Democratic'],\n",
       " ['bkicking xe2x80x98hiring red white amp youxe2x80x9d veterans job fair w txworkforce commissioner hughes minutemaidparks morn 150 employers w jobs  4000 expected veteranshappy families amp stronger texas #txlege httpstcokwjayi8cng',\n",
       "  'Democratic'],\n",
       " ['bexcited back bitter end nov 16 #songwriter #acoustic #guitar #thebitterend #livemusic httptcopoy7hax29r',\n",
       "  'Democratic'],\n",
       " ['bpotus says since election created 24 million new jobs including 200000 new jobs manufacturing alone #sotu',\n",
       "  'Republican'],\n",
       " ['bauthor atomic obsession john mueller expected join show tonight also lots heatlh care news today httpbitly1qq6qf',\n",
       "  'Republican'],\n",
       " ['bxe2x80x9cwhat ixe2x80x99ve learned since announced xe2x80x94 itxe2x80x99s five months back june xe2x80x94 people ready httpstcobjweysbcw1',\n",
       "  'Democratic'],\n",
       " ['bthoughts prayers great friends neighbors canada today #prayforottawa',\n",
       "  'Democratic'],\n",
       " ['bon #veteransday honoring memory veterans speaking laredo americanlegion #neverforget',\n",
       "  'Democratic'],\n",
       " ['b#disgraceful httpstco29t0odvas0', 'Democratic'],\n",
       " ['bfrom family #happythanksgiving thankful today httpstcogwtpoectzx',\n",
       "  'Republican']]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.seed(20201014)\n",
    "\n",
    "tweet_data_sample = random.choices(tweet_data,k=10)\n",
    "tweet_data_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With a word cutoff of 5, we have 55150 as features in the model.\n"
     ]
    }
   ],
   "source": [
    "# running feature set code again, for this data\n",
    "\n",
    "word_cutoff = 5\n",
    "\n",
    "tokens = [w for t, p in tweet_data for w in t.split()]\n",
    "\n",
    "word_dist = nltk.FreqDist(tokens)\n",
    "\n",
    "feature_words = set()\n",
    "\n",
    "for word, count in word_dist.items() :\n",
    "    if count > word_cutoff :\n",
    "        feature_words.add(word)\n",
    "        \n",
    "print(f\"With a word cutoff of {word_cutoff}, we have {len(feature_words)} as features in the model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresets_twit = [(conv_features(text,feature_words), party) for (text, party) in tweet_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's our (cleaned) tweet: {'bbigger': True, 'paychecks': True, 'xe2x86x92': True, 'realdonaldtrump': True, 'xe2x80x9cyour': True, 'going': True, 'way': True, 'taxes': True, 'right': True, 'first': True, 'time': True, 'long': True, 'youxe2x80x99ve': True, 'seen': True, 'coming': True, 'backxe2x80x9d': True}\n",
      "Actual party is Republican and our classifer says Republican.\n",
      "The classifier was correct\n",
      " \n",
      "Here's our (cleaned) tweet: {'b50': True, 'years': True, 'repjohnlewis': True, 'brave': True, 'foot': True, 'soldiers': True, 'risked': True, 'lives': True, 'voting': True, 'rights': True, 'bills': True, '#restorethevra': True, 'languishing': True}\n",
      "Actual party is Democratic and our classifer says Republican.\n",
      "The classifier was incorrect\n",
      " \n",
      "Here's our (cleaned) tweet: {'btogether': True, 'going': True, 'win': True, 'may': True, '8': True, 'work': True, 'create': True, 'future': True, 'children': True, 'west': True, 'virginia': True, 'join': True, 'team': True, '2': True, 'canvassing': True, 'huntington': True, 'wv': True}\n",
      "Actual party is Democratic and our classifer says Republican.\n",
      "The classifier was incorrect\n",
      " \n",
      "Here's our (cleaned) tweet: {'bwe': True, 'need': True, 'put': True, 'back': True, 'public': True, 'service': True, 'career': True, 'politicians': True, 'happy': True, 'forget': True, 'duties': True, 'turn': True, 'working': True, 'congress': True, 'nonstop': True, 'proud': True, 'stand': True, 'congressional': True, 'term': True, 'limits': True, 'government': True, 'work': True, 'us': True}\n",
      "Actual party is Republican and our classifer says Republican.\n",
      "The classifier was correct\n",
      " \n",
      "Here's our (cleaned) tweet: {'bare': True, 'coming': True, '#tx24': True, 'veterans': True, 'fair': True, 'november': True, '3rd': True, 'info': True, 'expect': True, 'gt': True}\n",
      "Actual party is Republican and our classifer says Republican.\n",
      "The classifier was correct\n",
      " \n",
      "Here's our (cleaned) tweet: {'ba': True, 'big': True, 'blue': True, 'wave': True, 'first': True, 'step': True, 'towards': True, 'getting': True, 'corrupting': True, 'influence': True, 'money': True, 'politics': True, 'making': True, 'government': True, 'accountable': True, 'voters': True}\n",
      "Actual party is Democratic and our classifer says Republican.\n",
      "The classifier was incorrect\n",
      " \n",
      "Here's our (cleaned) tweet: {'bmy': True, 'mom': True, 'battling': True, 'ms': True, '30': True, 'years': True, 'last': True, 'thing': True, 'want': True, 'fight': True, 'harder': True, 'pay': True, 'bills': True, 'shexe2x80x99s': True, 'refuse': True, 'stand': True, 'watch': True, 'republicans': True, 'cut': True, 'medicare': True, 'gives': True, 'independence': True, 'health': True}\n",
      "Actual party is Democratic and our classifer says Republican.\n",
      "The classifier was incorrect\n",
      " \n",
      "Here's our (cleaned) tweet: {'byou': True, 'view': True, 'seven': True, 'winners': True, 'including': True, 'one': True, 'voted': True, 'facebook': True, 'page': True, '#oh06': True}\n",
      "Actual party is Republican and our classifer says Republican.\n",
      "The classifier was correct\n",
      " \n",
      "Here's our (cleaned) tweet: {'bthis': True, 'election': True, 'stand': True, 'valley': True, 'fight': True, 'communities': True, 'delivered': True, 'keep': True, 'delivering': True, 'families': True, 'valadao': True, 'didnt': True, 'instead': True, 'voted': True, 'protections': True, 'preexisting': True}\n",
      "Actual party is Democratic and our classifer says Republican.\n",
      "The classifier was incorrect\n",
      " \n",
      "Here's our (cleaned) tweet: {'bsunrise': True, 'road': True, '#gotv': True, '#tx10': True}\n",
      "Actual party is Democratic and our classifer says Republican.\n",
      "The classifier was incorrect\n",
      " \n",
      "Here's our (cleaned) tweet: {'bthe': True, 'house': True, 'representatives': True, 'read': True, 'us': True, 'constitution': True, 'floor': True, 'today': True, 'first': True, 'time': True, 'history': True, 'watch': True, 'live': True, 'online': True, '11': True, 'et': True}\n",
      "Actual party is Republican and our classifer says Republican.\n",
      "The classifier was correct\n",
      " \n"
     ]
    }
   ],
   "source": [
    "limit = 10 # to only show 10\n",
    "c = 0\n",
    "\n",
    "for tweet, party in featuresets_twit :\n",
    "    \n",
    "    estimated_party = classifier.classify(tweet)\n",
    "    # Fill in the right-hand side above with code that estimates the actual party\n",
    "    \n",
    "    print(f\"Here's our (cleaned) tweet: {tweet}\")\n",
    "    print(f\"Actual party is {party} and our classifer says {estimated_party}.\")\n",
    "    if party == estimated_party:\n",
    "        print(\"The classifier was correct\")\n",
    "    else:\n",
    "        print(\"The classifier was incorrect\")\n",
    "    print(\" \")\n",
    "    \n",
    "    c += 1    \n",
    "    if c > limit: # to ensure it doesn't print all the tweets\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've looked at it some, let's score a bunch and see how we're doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary of counts by actual party and estimated party. \n",
    "# first key is actual, second is estimated\n",
    "parties = ['Republican','Democratic']\n",
    "results = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "for p in parties :\n",
    "    for p1 in parties :\n",
    "        results[p][p1] = 0\n",
    "\n",
    "\n",
    "num_to_score = 10000\n",
    "random.shuffle(tweet_data)\n",
    "\n",
    "for idx, tp in enumerate(featuresets_twit) :\n",
    "    tweet, party = tp    \n",
    "    # Now do the same thing as above, but we store the results rather\n",
    "    # than printing them. \n",
    "    \n",
    "    \n",
    "    estimated_party = classifier.classify(tweet)\n",
    "    \n",
    "    results[party][estimated_party] += 1\n",
    "    \n",
    "    if idx > num_to_score : \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.<lambda>()>,\n",
       "            {'Republican': defaultdict(int,\n",
       "                         {'Republican': 3595, 'Democratic': 778}),\n",
       "             'Democratic': defaultdict(int,\n",
       "                         {'Republican': 4633, 'Democratic': 996})})"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflections\n",
    "\n",
    "It appears that the classifier leans heavily towards Republicans on these tweets. One reason that may be the cause of some error in the application of the model is that politicians may not tweet like politicians give speeches. There are many more tweets than speeches given, as well, and they can be very one-off or completely unrelated to politics at times. And, perhaps, democrats tweet much more freely than they give speeches, using terms Republicans use, or even responding to Republicans, perhaps. Part of the twitter data has hashtags, links, and other typos and errors, which would need further cleaning to better match the training data. There was only very slight class imbalance; which may point towards the skew of these results, but not quite enough to fully explain it.\n",
    "\n",
    "Since it is a Naive Bayes classifier, it is going off the assumption that every pair of features is independent of each other, which is not the case for text or speech. As it's going off of probabilities of determining the class of a tweet based on its text, and the speech data was the training data, it goes back to the mismatch of the corpus in terms of the context of the actual text, and the different words likely used in speeches versus on twitter. Also, since republicans have much more of the 'higher importance' of features, if any of those words were used, I'd imagine the Naive Bayes model will lean heavily towards classifying it as Republican simply due to the probabilities. Again, perhaps Democrats utilized these words in response to Republicans and use their words against them, and/or they simply tweet more freely than they give speeches.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
